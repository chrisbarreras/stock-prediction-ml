{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "8Wbwl3O3Kh-X",
    "outputId": "f56bcd31-a5ef-41dc-c1f9-a11e251e7fb7"
   },
   "outputs": [],
   "source": "# Cell 1: Mount Drive and Load Data\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load dataset from Google Drive\ndataset = pd.read_csv('/content/drive/MyDrive/stock_prediction_data/processed_dataset.csv')\n\n# Sort by date for temporal splitting\ndataset['date'] = pd.to_datetime(dataset['date'])\ndataset = dataset.sort_values('date').reset_index(drop=True)\n\n# Use raw returns as target (better direction accuracy than excess returns)\ntarget_col = 'target_return'\n\nprint(f\"Loaded dataset: {dataset.shape}\")\nprint(f\"Samples: {len(dataset)}, Companies: {dataset['ticker'].nunique()}\")\nprint(f\"Date range: {dataset['date'].min().date()} to {dataset['date'].max().date()}\")\nprint(f\"Target: {target_col}\")\nprint(f\"  Mean: {dataset[target_col].mean():.4f}, Std: {dataset[target_col].std():.4f}\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 2: Feature Selection\nmeta_cols = ['ticker', 'date', 'target_return', 'target_excess_return', 'benchmark_return']\nall_feature_cols = [c for c in dataset.columns if c not in meta_cols]\n\nprint(f\"Starting features: {len(all_feature_cols)}\")\n\nX_raw = dataset[all_feature_cols].copy()\ny = dataset[target_col].copy()\n\n# 1. Drop features with zero variance\nvariances = X_raw.var()\nzero_var = variances[variances == 0].index.tolist()\nif zero_var:\n    print(f\"Dropping {len(zero_var)} zero-variance features: {zero_var}\")\n    X_raw = X_raw.drop(columns=zero_var)\n\n# 2. Drop highly correlated features (>0.95)\ncorr_matrix = X_raw.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\nhigh_corr_pairs = []\nto_drop = set()\nfor col in upper.columns:\n    correlated = upper.index[upper[col] > 0.95].tolist()\n    for c in correlated:\n        if c not in to_drop:\n            high_corr_pairs.append((col, c, corr_matrix.loc[col, c]))\n            to_drop.add(c)\n\nif to_drop:\n    print(f\"\\nDropping {len(to_drop)} highly correlated features (r > 0.95):\")\n    for col, c, r in high_corr_pairs:\n        print(f\"  {c} (corr={r:.3f} with {col})\")\n    X_raw = X_raw.drop(columns=list(to_drop))\n\nfeature_columns = list(X_raw.columns)\nprint(f\"\\nSelected features: {len(feature_columns)}\")\nprint(f\"Features: {feature_columns}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "2WjWDj48K733",
    "outputId": "3a783ef0-4e5f-4c79-df91-6278f265b399"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 3: Temporal Train/Test Split + Time-Series CV\n\nX = dataset[feature_columns].copy()\ny = dataset[target_col].copy()\n\n# Temporal split: train on earlier data, test on most recent 20%\nunique_dates = sorted(dataset['date'].unique())\nn_dates = len(unique_dates)\ncutoff_idx = int(n_dates * 0.8)\ncutoff_date = unique_dates[cutoff_idx]\n\ntrain_mask = dataset['date'] < cutoff_date\ntest_mask = dataset['date'] >= cutoff_date\n\nX_train_raw = X[train_mask]\nX_test_raw = X[test_mask]\ny_train = y[train_mask].values\ny_test = y[test_mask].values\n\nprint(f\"=== Temporal Split ===\")\nprint(f\"Cutoff date: {pd.Timestamp(cutoff_date).date()}\")\nprint(f\"Training: {len(X_train_raw)} samples ({dataset[train_mask]['date'].min().date()} to {dataset[train_mask]['date'].max().date()})\")\nprint(f\"Test:     {len(X_test_raw)} samples ({dataset[test_mask]['date'].min().date()} to {dataset[test_mask]['date'].max().date()})\")\n\n# Scale features\nscaler = StandardScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train_raw), columns=feature_columns)\nX_test = pd.DataFrame(scaler.transform(X_test_raw), columns=feature_columns)\n\n# Build expanding-window CV splits for training data\ntrain_dates = sorted(dataset[train_mask]['date'].unique())\nn_train_dates = len(train_dates)\nn_folds = 5\nfold_size = n_train_dates // (n_folds + 1)\n\ncv_splits = []\nfor i in range(n_folds):\n    train_end = (i + 2) * fold_size\n    val_start = train_end\n    val_end = min(train_end + fold_size, n_train_dates)\n\n    cv_train_dates = set(train_dates[:train_end])\n    cv_val_dates = set(train_dates[val_start:val_end])\n\n    train_idx = [j for j, d in enumerate(dataset[train_mask]['date']) if d in cv_train_dates]\n    val_idx = [j for j, d in enumerate(dataset[train_mask]['date']) if d in cv_val_dates]\n\n    if len(val_idx) > 0:\n        cv_splits.append((train_idx, val_idx))\n\nprint(f\"\\n=== Time-Series CV ({len(cv_splits)} folds) ===\")\nfor i, (tr, va) in enumerate(cv_splits):\n    print(f\"  Fold {i+1}: train={len(tr)} samples, val={len(va)} samples\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bj94aD0AK93A",
    "outputId": "d2de7cc8-afa3-41d6-b525-f771f1a48c1a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: Hyperparameter Tuning with Time-Series CV\n!pip install xgboost -q\n\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_distributions = {\n    'n_estimators': [100, 200, 300, 500],\n    'max_depth': [3, 4, 5, 6],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'subsample': [0.7, 0.8, 0.9],\n    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n    'min_child_weight': [1, 3, 5, 7],\n    'reg_alpha': [0, 0.01, 0.1, 1.0],\n    'reg_lambda': [1.0, 2.0, 5.0, 10.0],\n}\n\nbase_model = xgb.XGBRegressor(\n    random_state=42,\n    objective='reg:squarederror',\n)\n\nsearch = RandomizedSearchCV(\n    base_model,\n    param_distributions=param_distributions,\n    n_iter=50,\n    cv=cv_splits,\n    scoring='neg_root_mean_squared_error',\n    random_state=42,\n    n_jobs=-1,\n    verbose=1,\n)\n\nprint(\"Running hyperparameter search (50 iterations, 5-fold temporal CV)...\")\nsearch.fit(X_train.values, y_train)\n\nprint(f\"\\n=== Best Parameters ===\")\nfor k, v in search.best_params_.items():\n    print(f\"  {k}: {v}\")\nprint(f\"\\nBest CV RMSE: {-search.best_score_:.4f}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8jDIFp2LAK5",
    "outputId": "bb41aabf-0a76-40e8-fe64-e3e6e2c6a884"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Train Final Model and Evaluate\nmodel = search.best_estimator_\n\ny_pred_train = model.predict(X_train.values)\ny_pred_test = model.predict(X_test.values)\n\n# Regression metrics\ntrain_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\ntrain_r2 = r2_score(y_train, y_pred_train)\ntest_r2 = r2_score(y_test, y_pred_test)\ntest_mae = mean_absolute_error(y_test, y_pred_test)\n\n# Direction accuracy\ndirection_correct = np.sum(np.sign(y_test) == np.sign(y_pred_test))\ndirection_accuracy = direction_correct / len(y_test)\n\n# Cross-validation scores from search\ncv_rmse_mean = -search.best_score_\ncv_results = search.cv_results_\nbest_idx = search.best_index_\ncv_rmse_std = cv_results['std_test_score'][best_idx]\n\nprint(\"=\"*55)\nprint(\"    MODEL PERFORMANCE (Temporal Split)\")\nprint(\"=\"*55)\nprint(f\"\\nTarget: {target_col}\")\nprint(f\"Training: {len(X_train)} samples | Test: {len(X_test)} samples\")\nprint(f\"\\n--- Regression Metrics ---\")\nprint(f\"Train RMSE: {train_rmse:.4f}\")\nprint(f\"Test RMSE:  {test_rmse:.4f}\")\nprint(f\"Train R2:   {train_r2:.4f}\")\nprint(f\"Test R2:    {test_r2:.4f}\")\nprint(f\"Test MAE:   {test_mae:.4f}\")\nprint(f\"Overfit ratio: {train_rmse/test_rmse:.2f} (closer to 1.0 = less overfit)\")\nprint(f\"\\n--- Time-Series Cross-Validation ---\")\nprint(f\"CV RMSE:    {cv_rmse_mean:.4f} (+/- {abs(cv_rmse_std):.4f})\")\nprint(f\"\\n--- Direction Accuracy ---\")\nprint(f\"Correct direction: {direction_correct}/{len(y_test)} ({direction_accuracy:.1%})\")\nprint(f\"(Baseline: 50%)\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbUU3TeKLBsS",
    "outputId": "10db56e4-5b2c-4863-9c61-e632beedd103"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 6: Feature Importance\nimport matplotlib.pyplot as plt\n\nimportance = pd.DataFrame({\n    'feature': feature_columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=True)\n\nfig, ax = plt.subplots(figsize=(10, max(6, len(feature_columns) * 0.3)))\nax.barh(importance['feature'], importance['importance'], color='steelblue')\nax.set_xlabel('Feature Importance')\nax.set_title('XGBoost Feature Importance (After Selection)')\nplt.tight_layout()\nplt.savefig('/content/drive/MyDrive/stock_prediction_data/feature_importance.png', dpi=150)\nplt.show()\n\n# Show top and bottom features\nprint(\"Top 10 features:\")\nfor _, row in importance.tail(10).iloc[::-1].iterrows():\n    bar = '#' * int(row['importance'] * 50)\n    print(f\"  {row['feature']:30s} {row['importance']:.3f} {bar}\")\n\n# Identify low-importance features\nlow_imp = importance[importance['importance'] < 0.01]\nif len(low_imp) > 0:\n    print(f\"\\n{len(low_imp)} features with <1% importance (candidates for removal):\")\n    for _, row in low_imp.iterrows():\n        print(f\"  {row['feature']}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "vk0ouHQRLDOC",
    "outputId": "08126a87-b3b4-42e0-eea7-ea8f04c016a0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 7: Prediction Analysis\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. Predicted vs Actual\naxes[0, 0].scatter(y_test, y_pred_test, alpha=0.5, edgecolor='black', s=40)\nlims = [min(y_test.min(), y_pred_test.min()) - 0.05, max(y_test.max(), y_pred_test.max()) + 0.05]\naxes[0, 0].plot(lims, lims, 'r--', label='Perfect prediction')\naxes[0, 0].set_xlabel(f'Actual {target_col}')\naxes[0, 0].set_ylabel(f'Predicted {target_col}')\naxes[0, 0].set_title('Predicted vs Actual (Test Set)')\naxes[0, 0].legend()\n\n# 2. Residuals\nresiduals = y_test - y_pred_test\naxes[0, 1].hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\naxes[0, 1].axvline(x=0, color='red', linestyle='--')\naxes[0, 1].set_xlabel('Prediction Error')\naxes[0, 1].set_ylabel('Frequency')\naxes[0, 1].set_title('Distribution of Prediction Errors')\n\n# 3. Predictions over time\ntest_dates = dataset[test_mask]['date'].values\naxes[1, 0].scatter(test_dates, y_test, alpha=0.4, label='Actual', s=20)\naxes[1, 0].scatter(test_dates, y_pred_test, alpha=0.4, label='Predicted', s=20)\naxes[1, 0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\naxes[1, 0].set_xlabel('Date')\naxes[1, 0].set_ylabel(target_col)\naxes[1, 0].set_title('Predictions Over Time (Test Period)')\naxes[1, 0].legend()\naxes[1, 0].tick_params(axis='x', rotation=45)\n\n# 4. Cumulative returns (if we traded on predictions)\ntest_returns = dataset[test_mask]['target_return'].values\n# Strategy: go long if predicted excess return > 0, else hold cash\nif target_col == 'target_excess_return':\n    strategy_returns = np.where(y_pred_test > 0, test_returns, 0)\nelse:\n    strategy_returns = np.where(y_pred_test > 0, test_returns, 0)\nbuy_hold = np.cumprod(1 + test_returns) - 1\nstrategy = np.cumprod(1 + strategy_returns) - 1\naxes[1, 1].plot(range(len(buy_hold)), buy_hold, label='Buy & Hold', alpha=0.7)\naxes[1, 1].plot(range(len(strategy)), strategy, label='Model Strategy', alpha=0.7)\naxes[1, 1].set_xlabel('Test Sample')\naxes[1, 1].set_ylabel('Cumulative Return')\naxes[1, 1].set_title('Cumulative Returns: Model vs Buy & Hold')\naxes[1, 1].legend()\naxes[1, 1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.savefig('/content/drive/MyDrive/stock_prediction_data/model_results.png', dpi=150)\nplt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "joYeIuBzLE4t",
    "outputId": "c44a120d-4228-4af4-c436-8b2b52ae27a1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 8: Save Model and Results\nresults = {\n    'model': model,\n    'scaler': scaler,\n    'feature_columns': feature_columns,\n    'target_column': target_col,\n    'best_params': search.best_params_,\n    'metrics': {\n        'train_rmse': train_rmse,\n        'test_rmse': test_rmse,\n        'train_r2': train_r2,\n        'test_r2': test_r2,\n        'test_mae': test_mae,\n        'direction_accuracy': float(direction_accuracy),\n        'cv_rmse_mean': cv_rmse_mean,\n        'cv_rmse_std': float(abs(cv_rmse_std)),\n        'overfit_ratio': float(train_rmse / test_rmse),\n    },\n    'split_info': {\n        'method': 'temporal',\n        'cutoff_date': str(pd.Timestamp(cutoff_date).date()),\n        'train_size': len(X_train),\n        'test_size': len(X_test),\n        'cv_folds': len(cv_splits),\n    },\n    # Save predictions for local analysis (avoids XGBoost version mismatch)\n    'predictions': {\n        'y_train': y_train.tolist(),\n        'y_test': y_test.tolist(),\n        'y_pred_train': y_pred_train.tolist(),\n        'y_pred_test': y_pred_test.tolist(),\n        'test_tickers': dataset[test_mask]['ticker'].tolist(),\n        'test_dates': dataset[test_mask]['date'].astype(str).tolist(),\n        'test_raw_returns': dataset[test_mask]['target_return'].tolist(),\n    },\n}\n\nwith open('/content/drive/MyDrive/stock_prediction_data/model_results.pkl', 'wb') as f:\n    pickle.dump(results, f)\n\nprint(\"Saved to Google Drive: model_results.pkl\")\nprint(f\"\\n=== Final Results ===\")\nfor k, v in results['metrics'].items():\n    print(f\"  {k}: {v:.4f}\")\nprint(f\"\\n=== Split Info ===\")\nfor k, v in results['split_info'].items():\n    print(f\"  {k}: {v}\")\nprint(f\"\\n=== Best Hyperparameters ===\")\nfor k, v in results['best_params'].items():\n    print(f\"  {k}: {v}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kc7M0NALHRQ",
    "outputId": "479f1173-de01-4a4f-c34e-50837e530d20"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}